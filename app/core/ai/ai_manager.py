import os
import json
import glob
import asyncio
import re
from typing import List, Dict, Optional

from PyQt6.QtCore import QObject, pyqtSignal, QThread, QTimer, Qt

from app.config import AI_CTX_SIZE, AI_GPU_LAYERS, AI_SERVER_PORT, MODELS_DIR
from app.core.ai.server_manager import ServerManager
from app.core.ai.llama_client import LlamaClient
from app.core.ai.prompts import PromptBuilder, AnalysisPriority
from app.core.log_manager import logger

# --- –í–æ—Ä–∫–µ—Ä –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤ ---
class AIProcessingWorker(QThread):
    progress_value = pyqtSignal(int)
    result_signal = pyqtSignal(int, str, dict)
    finished_signal = pyqtSignal()
    error_signal = pyqtSignal(str)

    def __init__(self, port: int, items: List[Dict], prompts: List[str], rag_messages: List[Optional[str]], context: Dict, model_name: str):
        super().__init__()
        self.port = port
        self.items = items
        self.prompts = prompts
        self.rag_messages = rag_messages
        self.context = context
        self.model_name = model_name
        self._is_running = True

    def stop(self):
        self._is_running = False

    def run(self):
        asyncio.run(self._process_async())

    async def _process_async(self):
        client = LlamaClient(self.port)
        try:
            total = len(self.items)
            
            # --- –ù–ê–°–¢–†–û–ô–ö–ò –ì–ï–ù–ï–†–ê–¶–ò–ò (SMART PARAMS) ---
            gen_params = {
                "response_format": {"type": "json_object"}, 
                "temperature": 0.2,       # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Å—Ç—Ä–æ–≥–æ–π –ª–æ–≥–∏–∫–∏
                "top_k": 40,              # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤
                "top_p": 0.9,             # Nucleus sampling
                "repeat_penalty": 1.1,    # –ß—Ç–æ–±—ã –Ω–µ –∑–∞—Ü–∏–∫–ª–∏–≤–∞–ª—Å—è
                "max_tokens": 1024,
                # Mirostat –¥–µ–ª–∞–µ—Ç –æ—Ç–≤–µ—Ç—ã –±–æ–ª–µ–µ "—É–º–Ω—ã–º–∏" –∏ —Å–≤—è–∑–Ω—ã–º–∏, –∂–µ—Ä—Ç–≤—É—è —Å–∫–æ—Ä–æ—Å—Ç—å—é
                "mirostat_mode": 2,       
                "mirostat_tau": 5.0,
                "mirostat_eta": 0.1
            }

            for i, item in enumerate(self.items):
                if not self._is_running: break
                
                if i < len(self.rag_messages) and self.rag_messages[i]:
                    logger.success(self.rag_messages[i])

                logger.progress(f"–ù–µ–π—Ä–æ—Å–µ—Ç—å –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç: {i + 1}/{total}", token="ai_batch")
                self.progress_value.emit(int(((i + 1) / total) * 100))
                
                prompt_text = self.prompts[i] if i < len(self.prompts) else self.prompts[-1]
                
                # –ú–∏–Ω–∏-–¥–∞–º–ø –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤, —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–µ–µ
                clean_item = {k: v for k, v in item.items() if k in ['title', 'price', 'description', 'city', 'condition', 'seller_id']}
                item_dump = json.dumps(clean_item, ensure_ascii=False)
                
                messages = [
                    {"role": "system", "content": "–¢—ã ‚Äî —Å—Ç—Ä–æ–≥–∏–π —ç–∫—Å–ø–µ—Ä—Ç-—Å–∫—É–ø—â–∏–∫. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON."},
                    {"role": "user", "content": f"{prompt_text}\n\n–û–ë–™–Ø–í–õ–ï–ù–ò–ï:\n{item_dump}"}
                ]

                response = await client.chat_completion(
                    model=self.model_name,
                    messages=messages,
                    params=gen_params
                )

                if response:
                    cleaned = self._clean_json(response)
                    self.result_signal.emit(i, cleaned, self.context)
                else:
                    self.error_signal.emit(f"–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç AI –¥–ª—è #{i}")
            
            logger.success("–ê–Ω–∞–ª–∏–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é –∑–∞–≤–µ—Ä—à–µ–Ω")
            self.finished_signal.emit()

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ AI –≤–æ—Ä–∫–µ—Ä–∞: {e}")
            self.error_signal.emit(str(e))
        finally:
            await client.close()

    def _clean_json(self, text: str) -> str:
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ JSON –æ–±—ä–µ–∫—Ç, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –≤—ã–¥–∞–ª–∞ –ª–∏—à–Ω–∏–π —Ç–µ–∫—Å—Ç
        match = re.search(r'\{.*\}', text, re.DOTALL)
        if match:
            return match.group(0)
        return text.replace("```json", "").replace("```", "").strip()

# --- –í–æ—Ä–∫–µ—Ä –¥–ª—è —á–∞—Ç–∞ ---
class AIChatWorker(QThread):
    response_signal = pyqtSignal(str)

    def __init__(self, port: int, messages: List[Dict], model_name: str):
        super().__init__()
        self.port = port
        self.messages = messages
        self.model_name = model_name

    def run(self):
        asyncio.run(self._chat_async())

    async def _chat_async(self):
        client = LlamaClient(self.port)
        try:
            # –î–ª—è —á–∞—Ç–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º—è–≥—á–µ, —á—Ç–æ–±—ã –æ–Ω –±—ã–ª "–∫—Ä–µ–∞—Ç–∏–≤–Ω–µ–µ" –≤ –æ–±—â–µ–Ω–∏–∏
            chat_params = {
                "temperature": 0.7,
                "top_k": 50,
                "top_p": 0.95,
                "repeat_penalty": 1.1,
                "max_tokens": 2048
            }
            
            resp = await client.chat_completion(
                self.model_name, 
                self.messages, 
                params=chat_params
            )
            
            if resp:
                self.response_signal.emit(resp)
            else:
                self.response_signal.emit("–û—à–∏–±–∫–∞: —Å–µ—Ä–≤–µ—Ä –º–æ–ª—á–∏—Ç.")
        except Exception as e:
            self.response_signal.emit(f"–û—à–∏–±–∫–∞ —Å–≤—è–∑–∏: {e}")
        finally:
            await client.close()

class AIChunkCultivationWorker(QThread):
    """
    –í–æ—Ä–∫–µ—Ä –¥–ª—è –∫—É–ª—å—Ç–∏–≤–∞—Ü–∏–∏ –û–î–ù–û–ì–û —á–∞–Ω–∫–∞ –ø–∞–º—è—Ç–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–∞–∫ dict:
      - status: 'success' / 'error'
      - content: dict —Å JSON-–∫–æ–Ω—Ç–µ–Ω—Ç–æ–º —á–∞–Ω–∫–∞ (–ø—Ä–∏ —É—Å–ø–µ—Ö–µ)
      - summary: –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
      - error: —Ç–µ–∫—Å—Ç –æ—à–∏–±–∫–∏ (–ø—Ä–∏ –Ω–µ—É—Å–ø–µ—Ö–µ)
    """

    finished = pyqtSignal(dict)
    error_signal = pyqtSignal(str)

    def __init__(self, port: int, chunk_id: int, chunk_type: str,
                 memory_manager, model_name: str, prompt: str):
        super().__init__()
        self.port = port
        self.chunk_id = chunk_id
        self.chunk_type = chunk_type
        self.memory = memory_manager
        self.model_name = model_name
        self.prompt = prompt
        self._is_running = True

    def stop(self):
        self._is_running = False

    def run(self):
        asyncio.run(self._cultivate_chunk())

    async def _cultivate_chunk(self):
        client = LlamaClient(self.port)
        try:
            if not self._is_running:
                self.finished.emit({"status": "error", "error": "cancelled"})
                return

            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–∞–∫–∏–µ –∂–µ, –∫–∞–∫ –¥–ª—è –æ–±—ã—á–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
            gen_params = {
                "response_format": {"type": "json_object"},
                "temperature": 0.2,
                "top_k": 40,
                "top_p": 0.9,
                "repeat_penalty": 1.1,
                "max_tokens": 1024,
                "mirostat_mode": 2,
                "mirostat_tau": 5.0,
                "mirostat_eta": 0.1,
            }

            messages = [
                {
                    "role": "system",
                    "content": "–¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫ —Ä—ã–Ω–∫–∞. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–π —Å—Ö–µ–º–µ —á–∞–Ω–∫–∞.",
                },
                {"role": "user", "content": self.prompt},
            ]

            logger.progress(
                f"–ö—É–ª—å—Ç–∏–≤–∞—Ü–∏—è —á–∞–Ω–∫–∞ {self.chunk_id} ({self.chunk_type})...",
                token="ai-cult",
            )

            response = await client.chat_completion(
                model=self.model_name,
                messages=messages,
                params=gen_params,
            )

            if not response:
                msg = f"–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç AI –ø—Ä–∏ –∫—É–ª—å—Ç–∏–≤–∞—Ü–∏–∏ —á–∞–Ω–∫–∞ {self.chunk_id}"
                self.error_signal.emit(msg)
                self.finished.emit({"status": "error", "error": msg})
                return

            # –û—á–∏—Å—Ç–∫–∞ JSON —Ç–∞–∫–∞—è –∂–µ –∏–¥–µ—è, –∫–∞–∫ –≤ AIProcessingWorker
            text = response
            match = re.search(r"\{.*\}", text, re.DOTALL)
            if match:
                text = match.group(0)
            text = text.replace("``````", "").strip()

            try:
                data = json.loads(text)
            except Exception as e:
                err = f"JSON decode error for chunk {self.chunk_id}: {e}"
                logger.error(err, token="ai-cult")
                self.error_signal.emit(err)
                self.finished.emit({"status": "error", "error": str(e)})
                return

            # –ü—ã—Ç–∞–µ–º—Å—è –≤—ã—Ç–∞—â–∏—Ç—å summary –∏–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –ø–æ–ª–µ–π
            summary = None
            if isinstance(data, dict):
                summary = (
                    data.get("analysis", {}).get("summary")
                    if isinstance(data.get("analysis"), dict)
                    else None
                )
                if not summary:
                    summary = data.get("summary")

            result = {
                "status": "success",
                "content": data,
                "summary": summary,
            }
            self.finished.emit(result)
        except Exception as e:
            err = f"–û—à–∏–±–∫–∞ –∫—É–ª—å—Ç–∏–≤–∞—Ü–∏–∏ —á–∞–Ω–∫–∞ {self.chunk_id}: {e}"
            logger.error(err, token="ai-cult")
            self.error_signal.emit(str(e))
            self.finished.emit({"status": "error", "error": str(e)})
        finally:
            await client.close()

class AICultivationWorker(QThread):
    finished_signal = pyqtSignal()
    error_signal = pyqtSignal(str)
    
    def __init__(self, port, memory_manager, model_name):
        super().__init__()
        self.port = port
        self.memory = memory_manager
        self.model_name = model_name
        self._is_running = True

    def run(self):
        asyncio.run(self._cultivate())

    async def _cultivate(self):
        client = LlamaClient(self.port)
        try:
            # –ë–µ—Ä–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –≥–¥–µ –º–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤ (—Ç–æ–ø-10 –ø–æ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏)
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–π SQL –∫ MemoryManager, —Ç–∞–∫ –∫–∞–∫ –º–µ—Ç–æ–¥–∞ get_top –ø–æ–∫–∞ –Ω–µ—Ç
            raw_stats = self.memory.get_all_statistics(limit=15)
            
            processed_count = 0
            for st in raw_stats:
                if not self._is_running: break
                
                key = st['product_key']
                items = self.memory.find_similar_items(key, limit=50)
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–∞–±—Ä–∞–ª–æ—Å—å —Ö–æ—Ç—è –±—ã 5 —Ç–æ–≤–∞—Ä–æ–≤
                if len(items) < 5: continue 

                prompt = PromptBuilder.build_knowledge_prompt(key, items)
                if not prompt: continue

                logger.progress(f"–ö—É–ª—å—Ç–∏–≤–∞—Ü–∏—è –∑–Ω–∞–Ω–∏–π: {key}...", token="ai_cult")
                
                response = await client.chat_completion(
                    self.model_name,
                    [{"role": "user", "content": prompt}],
                    params={"response_format": {"type": "json_object"}, "temperature": 0.2}
                )

                if response:
                    try:
                        # ‚úÖ –£–õ–£–ß–®–ï–ù–ù–ê–Ø –æ—á–∏—Å—Ç–∫–∞ JSON
                        clean_json = response.strip()

                        # –£–¥–∞–ª–∏—Ç—å markdown –æ–±–µ—Ä—Ç–∫–∏
                        if clean_json.startswith("``````"):
                            clean_json = clean_json[7:-3].strip()
                        elif clean_json.startswith("``````"):
                            clean_json = clean_json[3:-3].strip()

                        # –£–¥–∞–ª–∏—Ç—å "json" –≤ –Ω–∞—á–∞–ª–µ –µ—Å–ª–∏ –µ—Å—Ç—å
                        if clean_json.lower().startswith("json"):
                            clean_json = clean_json[4:].strip()

                        # ‚úÖ –î–û–ë–ê–í–ò–¢–¨ - –ø–æ–ø—ã—Ç–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–µ–∑–∞–∫—Ä—ã—Ç—ã–µ –∫–∞–≤—ã—á–∫–∏
                        # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –æ–±—Ä—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–µ—Ä–µ–¥–∏–Ω–µ - –æ–±—Ä–µ–∑–∞—Ç—å –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –∑–∞–∫—Ä—ã—Ç–æ–π —Å–∫–æ–±–∫–∏
                        if clean_json.count('"') % 2 != 0:
                            logger.warning(f"–ù–µ–∑–∞–∫—Ä—ã—Ç—ã–µ –∫–∞–≤—ã—á–∫–∏ –≤ JSON –¥–ª—è {key}, –ø—ã—Ç–∞—é—Å—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å...", token="ai-cult")
                            # –ù–∞–π—Ç–∏ –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–∫—Ä—ã—Ç—É—é —Ñ–∏–≥—É—Ä–Ω—É—é —Å–∫–æ–±–∫—É
                            last_brace = clean_json.rfind('}')
                            if last_brace > 0:
                                clean_json = clean_json[:last_brace + 1]

                        data = json.loads(clean_json)

                        self.memory.add_knowledge(
                            product_key=key,
                            summary=data.get('summary', ''),
                            risks=data.get('risk_factors', ''),
                            prices=data.get('price_range_notes', '')
                        )
                        processed_count += 1
                        logger.success(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∑–Ω–∞–Ω–∏—è –¥–ª—è: {key}", token="ai-cult")

                    except json.JSONDecodeError as e:
                        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π –¥–ª—è {key}: {e}...", token="ai-cult")
                        logger.dev(f"–ü—Ä–æ–±–ª–µ–º–Ω—ã–π JSON: {clean_json[:500]}", level="ERROR")
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –¥–ª—è {key}: {e}...", token="ai-cult")

            if processed_count > 0:
                logger.success(f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –æ–±–Ω–æ–≤–ª–µ–Ω–∞: +{processed_count} –∑–∞–ø–∏—Å–µ–π", token="ai_cult")
            else:
                logger.info("–ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫—É–ª—å—Ç–∏–≤–∞—Ü–∏–∏", token="ai_cult")
                
            self.finished_signal.emit()

        except Exception as e:
            self.error_signal.emit(str(e))
        finally:
            await client.close()

# --- –ì–ª–∞–≤–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä (–û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä) ---
class AIManager(QObject):
    progress_signal = pyqtSignal(str)
    ai_progress_value = pyqtSignal(int)
    result_signal = pyqtSignal(int, str, dict)
    finished_signal = pyqtSignal()
    all_finished_signal = pyqtSignal()
    error_signal = pyqtSignal(str)
    server_ready_signal = pyqtSignal()
    chat_response_signal = pyqtSignal(str)

    def __init__(self, memory_manager=None):
        super().__init__()
        self.memory_manager = memory_manager
        self.current_model_path = self._find_default_model()
        self._model_name = os.path.basename(self.current_model_path) if self.current_model_path else "No Model"
        
        self.server_manager = ServerManager(self.current_model_path, port=AI_SERVER_PORT)
        self.server_manager.server_started.connect(self._on_server_started_process)
        self.server_manager.error_occurred.connect(self.error_signal.emit)
        
        self.processing_worker: Optional[AIProcessingWorker] = None
        self.chat_worker: Optional[AIChatWorker] = None
        
        self.health_timer = QTimer()
        self.health_timer.timeout.connect(self._check_health_and_notify)
        self._server_ready = False

        self._ctx_size = AI_CTX_SIZE
        self._gpu_layers = AI_GPU_LAYERS or -1
        self._gpu_device = 0
        self._backend = "auto"
        self._debug_logs = False

        self._chunk_workers: Dict[int, AIChunkCultivationWorker] = {}

    def _find_default_model(self) -> Optional[str]:
        if not os.path.exists(MODELS_DIR):
            os.makedirs(MODELS_DIR, exist_ok=True)
            return None
        files = glob.glob(os.path.join(MODELS_DIR, "*.gguf"))
        if files: return sorted(files)[0]
        return None

    def has_model(self) -> bool:
        return self.current_model_path and os.path.exists(self.current_model_path)
    
    def set_model(self, filename: str):
        path = os.path.join(MODELS_DIR, filename)
        if os.path.exists(path):
            self.current_model_path = path
            self._model_name = filename
            self.server_manager.set_model_path(path)
            if self.server_manager.is_running():
                self.server_manager.stop_server()

    def update_config(self, settings: dict):
        model_name = settings.get("ai_model")
        self._ctx_size = settings.get("ai_ctx_size", AI_CTX_SIZE)
        self._gpu_layers = settings.get("ai_gpu_layers", -1)
        self._gpu_device = settings.get("ai_gpu_device", 0)
        self._backend = settings.get("ai_backend", "auto")
        
        should_restart = False
        if model_name and model_name != self._model_name:
            self.set_model(model_name)
            should_restart = True

        if self.server_manager.is_running() or should_restart:
            if self.server_manager.is_running():
                 self.server_manager.stop_server()
            QTimer.singleShot(500, self.ensure_server)

    def ensure_server(self):
        if not self.has_model():
            self.error_signal.emit("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            return

        if not self.server_manager.is_running():
            self.progress_signal.emit("–ó–∞–ø—É—Å–∫ AI —Å–µ—Ä–≤–µ—Ä–∞...")
            self.server_manager.start_server(
                ctx_size=self._ctx_size, 
                gpu_layers=self._gpu_layers,
                gpu_device=self._gpu_device,
                backend_preference=self._backend
            )
        elif self._server_ready:
            self.server_ready_signal.emit()

    def _on_server_started_process(self):
        self.progress_signal.emit("–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏...")
        self.health_timer.start(1000)

    def _check_health_and_notify(self):
        import requests
        try:
            port = self.server_manager.get_port()
            logger.info(f"Checking AI server health on port {port}", token="ai-health")
            resp = requests.get(f"http://127.0.0.1:{port}/health", timeout=0.5)
            if resp.status_code == 200:
                self.health_timer.stop()
                self._server_ready = True
                logger.info("AI server is ready", token="ai-health")
                self.progress_signal.emit("AI –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
                self.server_ready_signal.emit()
            else:
                logger.error(f"AI server health check failed with status code {resp.status_code}", token="ai-health")
        except Exception as e:
            logger.error(f"AI server health check failed: {e}", token="ai-health")

    def start_processing(self, items: List[Dict], prompt: Optional[str], debug_mode: bool, context: Dict):
        self.ensure_server()
        if not self._server_ready:
            self.server_ready_signal.connect(lambda: self.start_processing(items, prompt, debug_mode, context), Qt.ConnectionType.SingleShotConnection)
            return

        prompts_list = []
        rag_messages_list = []

        if prompt: 
            prompts_list = [prompt] * len(items)
            rag_messages_list = [None] * len(items)
        else:
            # –ê–≤—Ç–æ-–ø—Ä–æ–º–ø—Ç —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º RAG
            prio = context.get('priority', 1)
            instr = context.get('user_instructions', "")
            
            for item in items:
                rag = None
                log_msg = None

                if self.memory_manager:
                    rag = self.memory_manager.get_rag_context_for_item(item.get('title', ''))

                if rag:
                    knowledge_text = rag.get('knowledge', '')
                    is_smart_chunk = knowledge_text and "–ù–µ—Ç –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ" not in knowledge_text

                    status_icon = "‚úÖ –ß–∞–Ω–∫ –∞–∫—Ç–∏–≤–µ–Ω" if is_smart_chunk else "‚ö†Ô∏è Live-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"
                    preview = knowledge_text[:40] + "..." if is_smart_chunk else "–û–ø–æ—Ä–∞ –Ω–∞ –º–∞—Ç. –æ–∂–∏–¥–∞–Ω–∏–µ"

                    stats_str = (
                        f"üìä {rag.get('sample_count', 0)} –ª–æ—Ç–æ–≤ | "
                        f"Med: {rag.get('median_price', 0)}‚ÇΩ | "
                        f"Avg: {rag.get('avg_price', 0)}‚ÇΩ"
                    )

                    log_msg = (
                        f"üß† –ü–ê–ú–Ø–¢–¨ ({item.get('title', '')[:20]}...):\n"
                        f"   ‚îî‚îÄ {stats_str}\n"
                        f"   ‚îî‚îÄ –†–µ–∂–∏–º: {status_icon} -> {preview}"
                    )

                rag_messages_list.append(log_msg)

                p = PromptBuilder.build_analysis_prompt(
                    items=[item], 
                    priority=prio, 
                    current_item=item, 
                    user_instructions=instr, 
                    rag_context=rag
                )
                prompts_list.append(p)

        if self.processing_worker and self.processing_worker.isRunning():
            self.processing_worker.stop()
            self.processing_worker.wait()

        self.processing_worker = AIProcessingWorker(
            port=self.server_manager.get_port(),
            items=items,
            prompts=prompts_list,
            rag_messages=rag_messages_list,
            context=context,
            model_name=self._model_name
        )
        self.processing_worker.progress_value.connect(self.ai_progress_value.emit)
        self.processing_worker.result_signal.connect(self.result_signal.emit)
        self.processing_worker.finished_signal.connect(self.finished_signal.emit)
        self.processing_worker.finished_signal.connect(self.all_finished_signal.emit)
        self.processing_worker.error_signal.connect(self.error_signal.emit)
        self.processing_worker.start()

    def start_cultivation_for_chunk(self, chunk_id: int, chunk_type: str,
                                    prompt: str, on_complete):
        """
        –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∫—É–ª—å—Ç–∏–≤–∞—Ü–∏–∏ –û–î–ù–û–ì–û —á–∞–Ω–∫–∞.
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è ChunkCultivationManager.
        """
        # 1. –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ —Å–µ—Ä–≤–µ—Ä –ø–æ–¥–Ω—è—Ç
        self.ensure_server()

        # –ï—Å–ª–∏ —Å–µ—Ä–≤–µ—Ä –µ—â—ë –ø–æ–¥–Ω–∏–º–∞–µ—Ç—Å—è ‚Äì –ø–æ–¥–æ–∂–¥–∞—Ç—å –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å
        if not self._server_ready:
            QTimer.singleShot(
                500,
                lambda: self.start_cultivation_for_chunk(
                    chunk_id, chunk_type, prompt, on_complete
                ),
            )
            logger.info(
                "AI server is starting, chunk cultivation will resume when ready...",
                token="ai-cult",
            )
            return

        # 2. –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –≤–æ—Ä–∫–µ—Ä –Ω–∞ —ç—Ç–æ—Ç —á–∞–Ω–∫
        port = self.server_manager.get_port()
        worker = AIChunkCultivationWorker(
            port=port,
            chunk_id=chunk_id,
            chunk_type=chunk_type,
            memory_manager=self.memory_manager,
            model_name=self._model_name,
            prompt=prompt,
        )
        self._chunk_workers[chunk_id] = worker

        def _on_finished(result: dict, cid=chunk_id):
            try:
                on_complete(result)
            finally:
                w = self._chunk_workers.pop(cid, None)
                if w is not None:
                    w.deleteLater()

        worker.finished.connect(_on_finished)
        worker.error_signal.connect(self.error_signal.emit)
        worker.start()

    def start_cultivation(self):
        # 1. –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –µ—Å—Ç—å –º–æ–¥–µ–ª—å –∏ —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω
        self.ensure_server()
        
        # –ï—Å–ª–∏ —Å–µ—Ä–≤–µ—Ä –µ—â—ë –ø–æ–¥–Ω–∏–º–∞–µ—Ç—Å—è ‚Äì –ø–æ–¥–æ–∂–¥–∞—Ç—å –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
        if not self._server_ready:
            # –ü–æ–≤—Ç–æ—Ä–Ω–æ –∑–∞–ø—É—Å—Ç–∏–º –∫—É–ª—å—Ç–∏–≤–∞—Ü–∏—é, –∫–æ–≥–¥–∞ —Å–µ—Ä–≤–µ—Ä —Å—Ç–∞–Ω–µ—Ç –≥–æ—Ç–æ–≤
            self.server_ready_signal.connect(
                lambda: self.start_cultivation(),
                Qt.ConnectionType.SingleShotConnection
            )
            # –ú—è–≥–∫–∏–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–∏–≥–Ω–∞–ª –≤ UI (–∞ –Ω–µ –æ—à–∏–±–∫–∞)
            logger.info("–ò–¥—ë—Ç –∑–∞–ø—É—Å–∫ AI —Å–µ—Ä–≤–µ—Ä–∞, –∫—É–ª—å—Ç–∏–≤–∞—Ü–∏—è –Ω–∞—á–Ω—ë—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏...", token="ai-cult")
            return
        
        # 2. –ï—Å–ª–∏ —É–∂–µ —á—Ç–æ-—Ç–æ –¥–µ–ª–∞–µ—Ç ‚Äì –Ω–µ –∑–∞–ø—É—Å–∫–∞—Ç—å –≤—Ç–æ—Ä—É—é –∑–∞–¥–∞—á—É
        if self.has_pending_tasks():
            logger.warning("–ò–ò –∑–∞–Ω—è—Ç –¥—Ä—É–≥–æ–π –∑–∞–¥–∞—á–µ–π...", token="ai-cult")
            self.error_signal.emit("–ò–ò –∑–∞–Ω—è—Ç –¥—Ä—É–≥–æ–π –∑–∞–¥–∞—á–µ–π")
            # –°—á–∏—Ç–∞–µ–º –æ–ø–µ—Ä–∞—Ü–∏—é ¬´–æ—Ç–º–µ–Ω—ë–Ω–Ω–æ–π¬ª ‚Äì —Å–æ–æ–±—â–∞–µ–º –æ–± –æ–∫–æ–Ω—á–∞–Ω–∏–∏
            self.all_finished_signal.emit()
            return
        
        # 3. –ù–æ—Ä–º–∞–ª—å–Ω—ã–π –ø—É—Ç—å: –∑–∞–ø—É—Å–∫–∞–µ–º –≤–æ—Ä–∫–µ—Ä –∫—É–ª—å—Ç–∏–≤–∞—Ü–∏–∏
        logger.info("–ó–∞–ø—É—Å–∫ –≤–æ—Ä–∫–µ—Ä–∞ –∫—É–ª—å—Ç–∏–≤–∞—Ü–∏–∏...", token="ai-cult")
        self.cultivation_worker = AICultivationWorker(
            port=self.server_manager.get_port(),
            memory_manager=self.memory_manager,
            model_name=self._model_name
        )
        # –ü–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –≤–æ—Ä–∫–µ—Ä–∞ –ø–æ–¥–Ω–∏–º–µ–º all_finished, —á—Ç–æ–±—ã UI —Ä–∞–∑–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª—Å—è
        self.cultivation_worker.finished_signal.connect(self.all_finished_signal.emit)
        self.cultivation_worker.error_signal.connect(self.error_signal.emit)
        self.cultivation_worker.start()

    def start_chat_request(self, messages: list, user_instructions: list = None):
        self.ensure_server()
        if not self._server_ready:
            self.server_ready_signal.connect(lambda: self.start_chat_request(messages, user_instructions), Qt.ConnectionType.SingleShotConnection)
            return
            
        if self.chat_worker and self.chat_worker.isRunning():
            self.chat_worker.wait()

        # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        sys_content = PromptBuilder.SYSTEM_BASE
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–ü–†–ò–ö–ê–ó–´)
        if user_instructions:
            rules = "\n".join([f"- {r}" for r in user_instructions])
            sys_content += f"\n\n[–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ò–ù–°–¢–†–£–ö–¶–ò–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø]:\n{rules}"

        # 2. –û–†–ö–ï–°–¢–†–ê–¢–û–†: –ê–Ω–∞–ª–∏–∑ –Ω–∞–º–µ—Ä–µ–Ω–∏—è
        last_msg = messages[-1]['content'].lower() if messages else ""
        # –¢—Ä–∏–≥–≥–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–∞ –æ —Ü–µ–Ω–µ/—Ä—ã–Ω–∫–µ
        is_market_query = any(w in last_msg for w in ['—Ü–µ–Ω–∞', '—Å–∫–æ–ª—å–∫–æ', '—Ä—ã–Ω–æ–∫', '—Å—Ç–æ–∏—Ç', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–ø–æ—á–µ–º', '–∞–Ω–∞–ª–∏–∑', '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', '—Å—Ä–µ–¥–Ω—è—è', '–º–µ–¥–∏–∞–Ω–∞'])
        
        rag_injection = ""
        if is_market_query and self.memory_manager:
            logger.info("–ß–∞—Ç: –ó–∞–ø—Ä–æ—Å –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø–∞–º—è—Ç–∏...", token="ai_chat")
            
            # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –∏—â–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ –≤—Å–µ–º—É —Ç–µ–∫—Å—Ç—É –∑–∞–ø—Ä–æ—Å–∞ (–æ–±—Ä–µ–∑–∞—è –ª–∏—à–Ω–µ–µ)
            search_key = last_msg[:60]
            rag_data = self.memory_manager.get_rag_context_for_item(search_key)
            
            if rag_data:
                rag_injection = (
                    f"\n\n[–î–ê–ù–ù–´–ï –ò–ó –ü–ê–ú–Ø–¢–ò –ü–û –ó–ê–ü–†–û–°–£]:\n"
                    f"–ù–∞–π–¥–µ–Ω–æ –ª–æ—Ç–æ–≤: {rag_data['sample_count']}\n"
                    f"–ú–µ–¥–∏–∞–Ω–Ω–∞—è —Ü–µ–Ω–∞: {rag_data['median_price']} —Ä—É–±.\n"
                    f"–¢—Ä–µ–Ω–¥: {rag_data.get('trend', 'N/A')}.\n"
                    f"–ó–Ω–∞–Ω–∏—è –ò–ò: {rag_data.get('knowledge', '–ù–µ—Ç')}\n"
                    f"–í–ê–ñ–ù–û: –ò–°–ü–û–õ–¨–ó–£–ô –≠–¢–ò –¶–ò–§–†–´ –î–õ–Ø –û–¢–í–ï–¢–ê."
                )
            else:
                rag_injection = "\n\n[–ü–ê–ú–Ø–¢–¨]: –î–∞–Ω–Ω—ã—Ö –ø–æ —ç—Ç–æ–º—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É —Ç–æ–≤–∞—Ä—É –≤ –±–∞–∑–µ –ø–æ–∫–∞ –Ω–µ—Ç. –ß–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º."

        MAX_HISTORY = 5
        trimmed_messages = messages[-MAX_HISTORY:] if len(messages) > MAX_HISTORY else messages

        MAX_MSG_LENGTH = 1000
        for msg in trimmed_messages:
            if len(msg.get('content', '')) > MAX_MSG_LENGTH:
                msg['content'] = msg['content'][:MAX_MSG_LENGTH] + "...[–æ–±—Ä–µ–∑–∞–Ω–æ]"

        final_messages = [{"role": "system", "content": sys_content + rag_injection}]
        for m in trimmed_messages:
            if m['role'] != 'system':
                final_messages.append(m)

        self.chat_worker = AIChatWorker(
            port=self.server_manager.get_port(),
            messages=final_messages,
            model_name=self._model_name
        )
        self.chat_worker.response_signal.connect(self.chat_response_signal.emit)
        self.chat_worker.start()
    
    def has_pending_tasks(self) -> bool:
        return (self.processing_worker and self.processing_worker.isRunning()) or (self.chat_worker and self.chat_worker.isRunning())

    def stop(self):
        if self.processing_worker:
            self.processing_worker.stop()
            self.processing_worker.wait()
            self.processing_worker = None
        if self.chat_worker:
            self.chat_worker.wait()
            self.chat_worker = None
        self.server_manager.stop_server()
        self.server_ready = False

    def refresh_resource_usage(self) -> dict:
        ram = self.server_manager.get_memory_info()
        return {
            "loaded": self._server_ready,
            "backend": self._backend,
            "model_name": self._model_name,
            "ram_mb": round(ram, 1),
            "vram_mb": 0.0, 
            "cpu_percent": 0.0,
            "gpu_percent": 0.0,
            "parser_eta_sec": 0,
            "ai_eta_sec": 0
        }
    
    def cleanup(self):
        self.stop()