import os
import json
import glob
import asyncio
import re
import requests
import gc
from typing import List, Dict, Optional

from PyQt6.QtCore import QObject, pyqtSignal, QThread, QTimer, Qt

from app.config import AI_CTX_SIZE, AI_GPU_LAYERS, AI_SERVER_PORT, MODELS_DIR
from app.core.ai.server_manager import ServerManager
from app.core.ai.llama_client import LlamaClient
from app.core.ai.prompts import PromptBuilder
from app.core.text_utils import TextMatcher
from app.core.log_manager import logger

# --- –í–æ—Ä–∫–µ—Ä –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤ ---
class AIProcessingWorker(QThread):
    progress_value = pyqtSignal(int)
    result_signal = pyqtSignal(int, str, dict)
    finished_signal = pyqtSignal()
    error_signal = pyqtSignal(str)

    def __init__(self, port: int, items: List[Dict], prompts: List[str], rag_messages: List[Optional[str]], context: Dict, model_name: str):
        super().__init__()
        self.port = port
        self.items = items
        self.prompts = prompts
        self.rag_messages = rag_messages
        self.context = context
        self.model_name = model_name
        self._is_running = True

    def stop(self):
        self._is_running = False
        TextMatcher.clear_cache()

    def run(self):
        asyncio.run(self._process_async())

    async def _process_async(self):
        client = LlamaClient(self.port)
        try:
            total = len(self.items)
            
            gen_params = {
                "response_format": {"type": "json_object"}, 
                "temperature": 0.2,
                "top_k": 40,
                "top_p": 0.9,
                "repeat_penalty": 1.1,
                "max_tokens": 1024,
                "mirostat_mode": 0,       
                #"mirostat_tau": 5.0,
                #"mirostat_eta": 0.1,
                #"cache_prompt": True
            }

            for i, item in enumerate(self.items):
                if not self._is_running: break
                
                if i < len(self.rag_messages) and self.rag_messages[i]:
                    logger.success(self.rag_messages[i])

                logger.progress(f"–ù–µ–π—Ä–æ—Å–µ—Ç—å –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç: {i + 1}/{total}", token="ai_batch")
                self.progress_value.emit(int(((i + 1) / total) * 100))
                
                prompt_text = self.prompts[i] if i < len(self.prompts) else self.prompts[-1]
                
                # –ú–∏–Ω–∏-–¥–∞–º–ø –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤, —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–µ–µ
                clean_item = {k: v for k, v in item.items() if k in ['title', 'price', 'description', 'city', 'condition', 'seller_id']}
                item_dump = json.dumps(clean_item, ensure_ascii=False)
                
                messages = [
                    {"role": "system", "content": "–¢—ã ‚Äî —Å—Ç—Ä–æ–≥–∏–π —ç–∫—Å–ø–µ—Ä—Ç-—Å–∫—É–ø—â–∏–∫. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON."},
                    {"role": "user", "content": f"{prompt_text}\n\n–û–ë–™–Ø–í–õ–ï–ù–ò–ï:\n{item_dump}"}
                ]

                response = await client.chat_completion(
                    model=self.model_name,
                    messages=messages,
                    params=gen_params
                )

                if response:
                    cleaned = self._clean_json(response)
                    self.result_signal.emit(i, cleaned, self.context)
                    if i % 5 == 0:
                        gc.collect()
                else:
                    self.error_signal.emit(f"–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç AI –¥–ª—è #{i}")
            
            logger.success("–ê–Ω–∞–ª–∏–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é –∑–∞–≤–µ—Ä—à–µ–Ω")
            self.finished_signal.emit()

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ AI –≤–æ—Ä–∫–µ—Ä–∞: {e}")
            self.error_signal.emit(str(e))
        finally:
            await client.close()
            TextMatcher.clear_cache()

    def _clean_json(self, text: str) -> str:
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ JSON –æ–±—ä–µ–∫—Ç, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –≤—ã–¥–∞–ª–∞ –ª–∏—à–Ω–∏–π —Ç–µ–∫—Å—Ç
        match = re.search(r'\{.*\}', text, re.DOTALL)
        if match:
            return match.group(0)
        return text.replace("```json", "").replace("```", "").strip()

# --- –í–æ—Ä–∫–µ—Ä –¥–ª—è —á–∞—Ç–∞ ---
class AIChatWorker(QThread):
    response_signal = pyqtSignal(str)

    def __init__(self, port: int, messages: List[Dict], model_name: str):
        super().__init__()
        self.port = port
        self.messages = messages
        self.model_name = model_name

    def run(self):
        asyncio.run(self._chat_async())

    async def _chat_async(self):
        client = LlamaClient(self.port)
        try:
            chat_params = {
                "temperature": 0.7,
                "top_k": 50,
                "top_p": 0.95,
                "repeat_penalty": 1.1,
                "max_tokens": 2048
            }
            
            resp = await client.chat_completion(
                self.model_name, 
                self.messages, 
                params=chat_params
            )
            
            if resp:
                self.response_signal.emit(resp)
            else:
                self.response_signal.emit("–û—à–∏–±–∫–∞: —Å–µ—Ä–≤–µ—Ä –º–æ–ª—á–∏—Ç.")
        except Exception as e:
            self.response_signal.emit(f"–û—à–∏–±–∫–∞ —Å–≤—è–∑–∏: {e}")
        finally:
            await client.close()

class AIChunkCultivationWorker(QThread):
    finished = pyqtSignal(dict)
    error_signal = pyqtSignal(str)

    def __init__(self, port: int, chunk_id: int, chunk_type: str,
                 memory_manager, model_name: str, prompt: str):
        super().__init__()
        self.port = port
        self.chunk_id = chunk_id
        self.chunk_type = chunk_type
        self.memory = memory_manager
        self.model_name = model_name
        self.prompt = prompt
        self._is_running = True

    def stop(self):
        self._is_running = False

    def run(self):
        asyncio.run(self._cultivate_chunk())

    async def _cultivate_chunk(self):
        client = LlamaClient(self.port)
        try:
            if hasattr(self, '_is_running') and not self._is_running:
                self.finished.emit({"status": "error", "error": "cancelled", "chunk_id": self.chunk_id})
                return

            gen_params = {
                "response_format": {"type": "json_object"},
                "temperature": 0.2,
                "top_k": 40,
                "top_p": 0.9,
                "repeat_penalty": 1.1,
                "max_tokens": 1024,
                "mirostat_mode": 2,
                "mirostat_tau": 5.0,
                "mirostat_eta": 0.1
            }

            messages = [
                {
                    "role": "system",
                    "content": "–¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫ —Ä—ã–Ω–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π Avito. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Å–≤–µ—Å—Ç–∏ –¥–∞–Ω–Ω—ã–µ –≤ –µ–¥–∏–Ω—ã–π JSON. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON –æ–±—ä–µ–∫—Ç–æ–º.",
                },
                {"role": "user", "content": self.prompt},
            ]

            logger.progress(
                f"–ö—É–ª—å—Ç–∏–≤–∞—Ü–∏—è —á–∞–Ω–∫–∞ {self.chunk_id} ({self.chunk_type})...",
                token="ai-cult",
            )

            response = await client.chat_completion(
                model=self.model_name,
                messages=messages,
                params=gen_params,
            )

            if not response or not isinstance(response, str) or len(response.strip()) < 10:
                msg = f"–ò–ò –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç –¥–ª—è —á–∞–Ω–∫–∞ {self.chunk_id}..."
                logger.error(msg, token="ai-cult")
                if hasattr(self, 'error_signal'):
                    self.error_signal.emit(msg)
                self.finished.emit({"status": "error", "error": msg, "chunk_id": self.chunk_id})
                return

            text = response.strip()
            
            if '```' in text:
                match_json = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', text, re.DOTALL)
                if match_json:
                    text = match_json.group(1)
                else:
                    parts = text.split('```')
                    if len(parts) > 1:
                        text = parts[1]
                        if text.strip().lower().startswith('json'):
                            text = text.strip()[4:].strip()

            match = re.search(r'\{.*\}', text, re.DOTALL)
            if not match:
                err = f"–í –æ—Ç–≤–µ—Ç–µ –ò–ò –Ω–µ –Ω–∞–π–¥–µ–Ω JSON-–æ–±—ä–µ–∫—Ç –¥–ª—è —á–∞–Ω–∫–∞ {self.chunk_id}..."
                logger.error(err, token="ai-cult")
                if hasattr(self, 'error_signal'):
                    self.error_signal.emit(err)
                self.finished.emit({"status": "error", "error": err, "chunk_id": self.chunk_id})
                return

            clean_json_text = match.group(0).strip()

            try:
                data = json.loads(clean_json_text)
            except json.JSONDecodeError as e:
                err = f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –¥–ª—è —á–∞–Ω–∫–∞ {self.chunk_id}: {str(e)}..."
                logger.error(err, token="ai-cult")
                if hasattr(self, 'error_signal'):
                    self.error_signal.emit(err)
                self.finished.emit({"status": "error", "error": err, "chunk_id": self.chunk_id})
                return

            summary = None
            if isinstance(data, dict):
                analysis = data.get("analysis", {})
                if isinstance(analysis, dict):
                    summary = analysis.get("summary")
                
                if not summary:
                    summary = data.get("summary")
            else:
                err = f"–ò–ò –≤–µ—Ä–Ω—É–ª –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —á–∞–Ω–∫–∞ {self.chunk_id}..."
                self.finished.emit({"status": "error", "error": err, "chunk_id": self.chunk_id})
                return

            result = {
                "status": "success",
                "content": data,
                "summary": summary if summary else "–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω",
                "chunk_id": self.chunk_id
            }
            self.finished.emit(result)

        except Exception as e:
            err = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —Å–±–æ–π –ø—Ä–∏ –∫—É–ª—å—Ç–∏–≤–∞—Ü–∏–∏ —á–∞–Ω–∫–∞ {self.chunk_id}: {str(e)}"
            logger.error(err, token="ai-cult", exc_info=True)
            if hasattr(self, 'error_signal'):
                self.error_signal.emit(str(e))
            self.finished.emit({"status": "error", "error": str(e), "chunk_id": self.chunk_id})
        finally:
            await client.close()

class AICultivationWorker(QThread):
    finished_signal = pyqtSignal()
    error_signal = pyqtSignal(str)
    
    def __init__(self, port, memory_manager, model_name):
        super().__init__()
        self.port = port
        self.memory = memory_manager
        self.model_name = model_name
        self._is_running = True

    def run(self):
        asyncio.run(self._cultivate())

    async def _cultivate(self):
        client = LlamaClient(self.port)
        try:
            raw_stats = self.memory.get_all_statistics(limit=15)
            
            processed_count = 0
            for st in raw_stats:
                if not self._is_running: break
                
                key = st['product_key']
                items = self.memory.find_similar_items(key, limit=20)
                
                if len(items) < 5: continue 

                prompt = PromptBuilder.build_knowledge_prompt(key, items)
                if not prompt: continue

                logger.progress(f"–ö—É–ª—å—Ç–∏–≤–∞—Ü–∏—è –∑–Ω–∞–Ω–∏–π: {key}...", token="ai_cult")
                
                response = await client.chat_completion(
                    self.model_name,
                    [{"role": "user", "content": prompt}],
                    params={"response_format": {"type": "json_object"}, "temperature": 0.2}
                )

                if response:
                    try:
                        clean_json = response.strip()

                        if clean_json.startswith("``````"):
                            clean_json = clean_json[7:-3].strip()
                        elif clean_json.startswith("``````"):
                            clean_json = clean_json[3:-3].strip()

                        if clean_json.lower().startswith("json"):
                            clean_json = clean_json[4:].strip()

                        if clean_json.count('"') % 2 != 0:
                            logger.warning(f"–ù–µ–∑–∞–∫—Ä—ã—Ç—ã–µ –∫–∞–≤—ã—á–∫–∏ –≤ JSON –¥–ª—è {key}, –ø—ã—Ç–∞—é—Å—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å...", token="ai-cult")
                            last_brace = clean_json.rfind('}')
                            if last_brace > 0:
                                clean_json = clean_json[:last_brace + 1]

                        data = json.loads(clean_json)

                        self.memory.add_knowledge(
                            product_key=key,
                            summary=data.get('summary', ''),
                            risks=data.get('risk_factors', ''),
                            prices=data.get('price_range_notes', '')
                        )
                        processed_count += 1
                        logger.success(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∑–Ω–∞–Ω–∏—è –¥–ª—è: {key}", token="ai-cult")

                    except json.JSONDecodeError as e:
                        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π –¥–ª—è {key}: {e}...", token="ai-cult")
                        logger.dev(f"–ü—Ä–æ–±–ª–µ–º–Ω—ã–π JSON: {clean_json[:500]}", level="ERROR")
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –¥–ª—è {key}: {e}...", token="ai-cult")

            if processed_count > 0:
                logger.success(f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –æ–±–Ω–æ–≤–ª–µ–Ω–∞: +{processed_count} –∑–∞–ø–∏—Å–µ–π", token="ai_cult")
            else:
                logger.info("–ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫—É–ª—å—Ç–∏–≤–∞—Ü–∏–∏", token="ai_cult")
                
            self.finished_signal.emit()

        except Exception as e:
            self.error_signal.emit(str(e))
        finally:
            await client.close()


class AIManager(QObject):
    progress_signal = pyqtSignal(str)
    ai_progress_value = pyqtSignal(int)
    result_signal = pyqtSignal(int, str, dict)
    finished_signal = pyqtSignal()
    all_finished_signal = pyqtSignal()
    error_signal = pyqtSignal(str)
    server_ready_signal = pyqtSignal()
    chat_response_signal = pyqtSignal(str)

    def __init__(self, memory_manager=None):
        super().__init__()
        self.memory_manager = memory_manager
        self.current_model_path = self._find_default_model()
        self._model_name = os.path.basename(self.current_model_path) if self.current_model_path else "No Model"
        
        self.server_manager = ServerManager(self.current_model_path, port=AI_SERVER_PORT)
        self.server_manager.server_started.connect(self._on_server_started_process)
        self.server_manager.error_occurred.connect(self.error_signal.emit)
        
        self.processing_worker: Optional[AIProcessingWorker] = None
        self.chat_worker: Optional[AIChatWorker] = None
        
        self.health_timer = QTimer()
        self.health_timer.timeout.connect(self._check_health_and_notify)
        self._server_ready = False

        self._ctx_size = AI_CTX_SIZE
        self._gpu_layers = AI_GPU_LAYERS or -1
        self._gpu_device = 0
        self._backend = "auto"
        self._debug_logs = False

        self._chunk_workers: Dict[int, AIChunkCultivationWorker] = {}
        self._cultivation_queue = []
        self._is_cultivating_now = False

    def _find_default_model(self) -> Optional[str]:
        if not os.path.exists(MODELS_DIR):
            os.makedirs(MODELS_DIR, exist_ok=True)
            return None
        files = glob.glob(os.path.join(MODELS_DIR, "*.gguf"))
        if files: return sorted(files)[0]
        return None

    def has_model(self) -> bool:
        return self.current_model_path and os.path.exists(self.current_model_path)
    
    def set_model(self, filename: str):
        path = os.path.join(MODELS_DIR, filename)
        if os.path.exists(path):
            self.current_model_path = path
            self._model_name = filename
            self.server_manager.set_model_path(path)
            if self.server_manager.is_running():
                self.server_manager.stop_server()

    def update_config(self, settings: dict):
        model_name = settings.get("ai_model")
        self._ctx_size = settings.get("ai_ctx_size", AI_CTX_SIZE)
        self._gpu_layers = settings.get("ai_gpu_layers", -1)
        self._gpu_device = settings.get("ai_gpu_device", 0)
        self._backend = settings.get("ai_backend", "auto")
        
        should_restart = False
        if model_name and model_name != self._model_name:
            self.set_model(model_name)
            should_restart = True

        if self.server_manager.is_running() or should_restart:
            if self.server_manager.is_running():
                 self.server_manager.stop_server()
            QTimer.singleShot(500, self.ensure_server)

    def ensure_server(self):
        if not self.has_model():
            self.error_signal.emit("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            return

        if not self.server_manager.is_running():
            self.progress_signal.emit("–ó–∞–ø—É—Å–∫ AI —Å–µ—Ä–≤–µ—Ä–∞...")
            self.server_manager.start_server(
                ctx_size=self._ctx_size, 
                gpu_layers=self._gpu_layers,
                gpu_device=self._gpu_device,
                backend_preference=self._backend
            )
        elif not self._server_ready:
            # –°–µ—Ä–≤–µ—Ä –∂–∏–≤, –Ω–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–µ –≤–∏–¥–∏—Ç –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏. –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É.
            if not self.health_timer.isActive():
                self.progress_signal.emit("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏...")
                self.health_timer.start(1000)
        elif self._server_ready:
            self.server_ready_signal.emit()

    def _on_server_started_process(self):
        self.progress_signal.emit("–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏...")
        self.health_timer.start(1000)

    def _check_health_and_notify(self):
        port = self.server_manager.get_port()
        try:
            # proxies={} –≤–∞–∂–Ω–æ –¥–ª—è –æ–±—Ö–æ–¥–∞ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö VPN/Proxy
            resp = requests.get(
                f"http://127.0.0.1:{port}/health",
                timeout=5.0,
                proxies={"http": None, "https": None}
            )
            
            if resp.status_code == 200:
                self._server_ready = True
                self.server_ready_signal.emit()
                self.server_manager._is_starting = False
                self.health_timer.stop()
                logger.success("AI –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ (Health OK)", token="ai-manager")
            elif resp.status_code == 503:
                self.progress_signal.emit("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –≤ –ø–∞–º—è—Ç—å...")
                logger.dev("AI: 503 Loading...", level="DEBUG")
            else:
                logger.warning(f"AI –æ—Ç–≤–µ—Ç–∏–ª –∫–æ–¥–æ–º: {resp.status_code}", token="ai-manager")
        
        except requests.exceptions.ConnectionError:
            logger.dev("AI: –ù–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å 127.0.0.1 (–ø–æ—Ä—Ç –µ—â–µ –∑–∞–∫—Ä—ã—Ç)", level="DEBUG")
        except Exception as e:
            logger.error(f"AI Health –æ—à–∏–±–∫–∞: {e}")

    def start_processing(self, items: List[Dict], prompt: Optional[str], debug_mode: bool, context: Dict):
        self.ensure_server()
        if not self._server_ready:
            self.server_ready_signal.connect(lambda: self.start_processing(items, prompt, debug_mode, context), Qt.ConnectionType.SingleShotConnection)
            return
        
        if not prompt:
            logger.info("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...", token="text_match")
            TextMatcher.precompute_corpus(items)

        prompts_list = []
        rag_messages_list = []

        search_mode = context.get('search_mode', 'full')

        if prompt: 
            prompts_list = [prompt] * len(items)
            rag_messages_list = [None] * len(items)
        else:
            prio = context.get('priority', 1)
            instr = context.get('user_instructions', "")
            
            for item in items:
                rag = None
                log_msg = None

                if self.memory_manager:
                    rag = self.memory_manager.get_rag_context_for_item(item.get('title', ''))

                if rag:
                    knowledge_text = rag.get('knowledge', '')
                    is_smart_chunk = knowledge_text and "–ù–µ—Ç –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ" not in knowledge_text

                    status_icon = "‚úÖ –ß–∞–Ω–∫ –∞–∫—Ç–∏–≤–µ–Ω" if is_smart_chunk else "‚ö†Ô∏è Live-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"
                    preview = knowledge_text[:40] + "..." if is_smart_chunk else "–û–ø–æ—Ä–∞ –Ω–∞ –º–∞—Ç. –æ–∂–∏–¥–∞–Ω–∏–µ"

                    stats_str = (
                        f"üìä {rag.get('sample_count', 0)} –ª–æ—Ç–æ–≤ | "
                        f"Med: {rag.get('median_price', 0)}‚ÇΩ | "
                        f"Avg: {rag.get('avg_price', 0)}‚ÇΩ"
                    )

                    log_msg = (
                        f"üß† –ü–ê–ú–Ø–¢–¨ ({item.get('title', '')[:20]}...):\n"
                        f"   ‚îî‚îÄ {stats_str}\n"
                        f"   ‚îî‚îÄ –†–µ–∂–∏–º: {status_icon} -> {preview}"
                    )

                rag_messages_list.append(log_msg)

                similar_items = TextMatcher.filter_similar_items(
                    target_title=item.get('title', ''), 
                    all_items=items,
                    threshold=0.35
                )

                p = PromptBuilder.build_analysis_prompt(
                    items=similar_items,
                    priority=prio, 
                    current_item=item, 
                    user_instructions=instr, 
                    rag_context=rag,
                    search_mode=search_mode
                )
                prompts_list.append(p)

        if self.processing_worker and self.processing_worker.isRunning():
            self.processing_worker.stop()
            self.processing_worker.wait()

        self.processing_worker = AIProcessingWorker(
            port=self.server_manager.get_port(),
            items=items,
            prompts=prompts_list,
            rag_messages=rag_messages_list,
            context=context,
            model_name=self._model_name
        )
        self.processing_worker.progress_value.connect(self.ai_progress_value.emit)
        self.processing_worker.result_signal.connect(self.result_signal.emit)
        self.processing_worker.finished_signal.connect(self.finished_signal.emit)
        self.processing_worker.finished_signal.connect(self.all_finished_signal.emit)
        self.processing_worker.error_signal.connect(self.error_signal.emit)
        self.processing_worker.start()

    def start_cultivation_for_chunk(self, chunk_id, chunk_type, prompt, on_complete):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —á–∞–Ω–∫ –≤ –æ—á–µ—Ä–µ–¥—å –Ω–∞ –∫—É–ª—å—Ç–∏–≤–∞—Ü–∏—é –≤–º–µ—Å—Ç–æ –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞.
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —É–∂–µ —ç—Ç–æ–≥–æ —á–∞–Ω–∫–∞ –≤ –æ—á–µ—Ä–µ–¥–∏
        if any(item['id'] == chunk_id for item in self._cultivation_queue):
            return

        self._cultivation_queue.append({
            "id": chunk_id,
            "type": chunk_type,
            "prompt": prompt,
            "callback": on_complete
        })
        
        logger.info(f"–ß–∞–Ω–∫ {chunk_id} –¥–æ–±–∞–≤–ª–µ–Ω –≤ –æ—á–µ—Ä–µ–¥—å (–≤—Å–µ–≥–æ: {len(self._cultivation_queue)})", token="ai-cult")
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—á–µ—Ä–µ–¥–∏
        QTimer.singleShot(100, self._process_cultivation_queue)

    def _process_cultivation_queue(self):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—á–µ—Ä–µ–¥—å –∫—É–ª—å—Ç–∏–≤–∞—Ü–∏–∏ —Å—Ç—Ä–æ–≥–æ –ø–æ –æ–¥–Ω–æ–º—É —á–∞–Ω–∫—É.
        """
        if self._is_cultivating_now:
            return # –£–∂–µ —á—Ç–æ-—Ç–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è

        if not self._cultivation_queue:
            return # –û—á–µ—Ä–µ–¥—å –ø—É—Å—Ç–∞

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–µ—Ä–∞
        if not self._server_ready:
            self.ensure_server()
            # –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ —Å–µ–∫—É–Ω–¥—É, –ø–æ–∫–∞ —Å–µ—Ä–≤–µ—Ä –Ω–µ –ø—Ä–æ–≥—Ä–µ–µ—Ç—Å—è
            QTimer.singleShot(1000, self._process_cultivation_queue)
            return

        self._is_cultivating_now = True
        task = self._cultivation_queue.pop(0)
        
        chunk_id = task["id"]
        logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑ –æ—á–µ—Ä–µ–¥–∏: —á–∞–Ω–∫ {chunk_id}", token="ai-cult")

        port = self.server_manager.get_port()
        worker = AIChunkCultivationWorker(
            port=port,
            chunk_id=chunk_id,
            chunk_type=task["type"],
            memory_manager=self.memory_manager,
            model_name=self._model_name,
            prompt=task["prompt"]
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –≤–æ—Ä–∫–µ—Ä, —á—Ç–æ–±—ã –µ–≥–æ –Ω–µ —Å—ä–µ–ª —Å–±–æ—Ä—â–∏–∫ –º—É—Å–æ—Ä–∞
        self._chunk_workers[chunk_id] = worker

        def _handle_finished(result: dict):
            try:
                task["callback"](result)
            finally:
                # –û—á–∏—Å—Ç–∫–∞ –∏ –∑–∞–ø—É—Å–∫ —Å–ª–µ–¥—É—é—â–µ–≥–æ
                w = self._chunk_workers.pop(chunk_id, None)
                if w:
                    w.quit()
                    w.wait()
                    w.deleteLater()
                
                self._is_cultivating_now = False
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ 1 —Å–µ–∫, —á—Ç–æ–±—ã –¥–∞—Ç—å GPU "–æ—Ç–¥—ã—à–∞—Ç—å—Å—è"
                QTimer.singleShot(1000, self._process_cultivation_queue)

        worker.finished.connect(_handle_finished)
        worker.error_signal.connect(self.error_signal.emit)
        worker.start()

    def start_cultivation(self):
        self.ensure_server()
        
        if not self._server_ready:
            self.server_ready_signal.connect(
                lambda: self.start_cultivation(),
                Qt.ConnectionType.SingleShotConnection
            )
            logger.info("–ò–¥—ë—Ç –∑–∞–ø—É—Å–∫ AI —Å–µ—Ä–≤–µ—Ä–∞, –∫—É–ª—å—Ç–∏–≤–∞—Ü–∏—è –Ω–∞—á–Ω—ë—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏...", token="ai-cult")
            return
        
        if self.has_pending_tasks():
            logger.warning("–ò–ò –∑–∞–Ω—è—Ç –¥—Ä—É–≥–æ–π –∑–∞–¥–∞—á–µ–π...", token="ai-cult")
            self.error_signal.emit("–ò–ò –∑–∞–Ω—è—Ç –¥—Ä—É–≥–æ–π –∑–∞–¥–∞—á–µ–π")
            # –°—á–∏—Ç–∞–µ–º –æ–ø–µ—Ä–∞—Ü–∏—é ¬´–æ—Ç–º–µ–Ω—ë–Ω–Ω–æ–π¬ª ‚Äì —Å–æ–æ–±—â–∞–µ–º –æ–± –æ–∫–æ–Ω—á–∞–Ω–∏–∏
            self.all_finished_signal.emit()
            return
        
        logger.info("–ó–∞–ø—É—Å–∫ –≤–æ—Ä–∫–µ—Ä–∞ –∫—É–ª—å—Ç–∏–≤–∞—Ü–∏–∏...", token="ai-cult")
        self.cultivation_worker = AICultivationWorker(
            port=self.server_manager.get_port(),
            memory_manager=self.memory_manager,
            model_name=self._model_name
        )
        self.cultivation_worker.finished_signal.connect(self.all_finished_signal.emit)
        self.cultivation_worker.error_signal.connect(self.error_signal.emit)
        self.cultivation_worker.start()

    def start_chat_request(self, messages: list, user_instructions: list = None):
        self.ensure_server()
        if not self._server_ready:
            self.server_ready_signal.connect(lambda: self.start_chat_request(messages, user_instructions), Qt.ConnectionType.SingleShotConnection)
            return
            
        if self.chat_worker and self.chat_worker.isRunning():
            self.chat_worker.wait()

        # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        sys_content = PromptBuilder.SYSTEM_BASE
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–ü–†–ò–ö–ê–ó–´)
        if user_instructions:
            rules = "\n".join([f"- {r}" for r in user_instructions])
            sys_content += f"\n\n[–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ò–ù–°–¢–†–£–ö–¶–ò–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø]:\n{rules}"

        # 2. –û–†–ö–ï–°–¢–†–ê–¢–û–†: –ê–Ω–∞–ª–∏–∑ –Ω–∞–º–µ—Ä–µ–Ω–∏—è
        last_msg = messages[-1]['content'].lower() if messages else ""
        # –¢—Ä–∏–≥–≥–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–∞ –æ —Ü–µ–Ω–µ/—Ä—ã–Ω–∫–µ
        is_market_query = any(w in last_msg for w in ['—Ü–µ–Ω–∞', '—Å–∫–æ–ª—å–∫–æ', '—Ä—ã–Ω–æ–∫', '—Å—Ç–æ–∏—Ç', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–ø–æ—á–µ–º', '–∞–Ω–∞–ª–∏–∑', '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', '—Å—Ä–µ–¥–Ω—è—è', '–º–µ–¥–∏–∞–Ω–∞'])
        
        rag_injection = ""
        if is_market_query and self.memory_manager:
            logger.info("–ß–∞—Ç: –ó–∞–ø—Ä–æ—Å –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø–∞–º—è—Ç–∏...", token="ai_chat")
            
            # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –∏—â–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ –≤—Å–µ–º—É —Ç–µ–∫—Å—Ç—É –∑–∞–ø—Ä–æ—Å–∞ (–æ–±—Ä–µ–∑–∞—è –ª–∏—à–Ω–µ–µ)
            search_key = last_msg[:60]
            rag_data = self.memory_manager.get_rag_context_for_item(search_key)
            
            if rag_data:
                rag_injection = (
                    f"\n\n[–î–ê–ù–ù–´–ï –ò–ó –ü–ê–ú–Ø–¢–ò –ü–û –ó–ê–ü–†–û–°–£]:\n"
                    f"–ù–∞–π–¥–µ–Ω–æ –ª–æ—Ç–æ–≤: {rag_data['sample_count']}\n"
                    f"–ú–µ–¥–∏–∞–Ω–Ω–∞—è —Ü–µ–Ω–∞: {rag_data['median_price']} —Ä—É–±.\n"
                    f"–¢—Ä–µ–Ω–¥: {rag_data.get('trend', 'N/A')}.\n"
                    f"–ó–Ω–∞–Ω–∏—è –ò–ò: {rag_data.get('knowledge', '–ù–µ—Ç')}\n"
                    f"–í–ê–ñ–ù–û: –ò–°–ü–û–õ–¨–ó–£–ô –≠–¢–ò –¶–ò–§–†–´ –î–õ–Ø –û–¢–í–ï–¢–ê."
                )
            else:
                rag_injection = "\n\n[–ü–ê–ú–Ø–¢–¨]: –î–∞–Ω–Ω—ã—Ö –ø–æ —ç—Ç–æ–º—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É —Ç–æ–≤–∞—Ä—É –≤ –±–∞–∑–µ –ø–æ–∫–∞ –Ω–µ—Ç. –ß–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º."

        MAX_HISTORY = 5
        trimmed_messages = messages[-MAX_HISTORY:] if len(messages) > MAX_HISTORY else messages

        MAX_MSG_LENGTH = 1000
        for msg in trimmed_messages:
            if len(msg.get('content', '')) > MAX_MSG_LENGTH:
                msg['content'] = msg['content'][:MAX_MSG_LENGTH] + "...[–æ–±—Ä–µ–∑–∞–Ω–æ]"

        final_messages = [{"role": "system", "content": sys_content + rag_injection}]
        for m in trimmed_messages:
            if m['role'] != 'system':
                final_messages.append(m)

        self.chat_worker = AIChatWorker(
            port=self.server_manager.get_port(),
            messages=final_messages,
            model_name=self._model_name
        )
        self.chat_worker.response_signal.connect(self.chat_response_signal.emit)
        self.chat_worker.start()
    
    def has_pending_tasks(self) -> bool:
        return (self.processing_worker and self.processing_worker.isRunning()) or (self.chat_worker and self.chat_worker.isRunning())

    def stop(self):
        if self.processing_worker:
            self.processing_worker.stop()
            self.processing_worker.wait()
            self.processing_worker = None
        if self.chat_worker:
            self.chat_worker.wait()
            self.chat_worker = None
        self.server_manager.stop_server()
        self._server_ready = False

    def refresh_resource_usage(self) -> dict:
        ram = self.server_manager.get_memory_info()
        return {
            "loaded": self._server_ready,
            "backend": self._backend,
            "model_name": self._model_name,
            "ram_mb": round(ram, 1),
            "vram_mb": 0.0, 
            "cpu_percent": 0.0,
            "gpu_percent": 0.0,
            "parser_eta_sec": 0,
            "ai_eta_sec": 0
        }
    
    def cleanup(self):
        self.stop()